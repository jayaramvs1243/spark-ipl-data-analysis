{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a02ca0ae-1b28-4ecc-9389-c8093888018a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## IPL Data Analysis\n",
    "\n",
    "Dataset from https://data.world/raghu543/ipl-data-till-2017\n",
    "\n",
    "This data set has the ball by ball data of all the Indian Premier League (IPL) matches till 2017 season."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df36f976-025d-4acd-ab40-9de0a9912738",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Note!!!**\n",
    "\n",
    "This notebook can be executed without any modifications in Databricks or Azure Databricks compute clusters. To access ADLS Gen2 from a local Spark cluster, you need to configure Spark with the necessary libraries and credentials.\n",
    "\n",
    "***`Setting PYSPARK_SUBMIT_ARGS - Per Session`***\n",
    "\n",
    "**Install the Hadoop Azure Libraries:**\n",
    "\n",
    "You can use pip to install the Hadoop Azure libraries directly from a Jupyter notebook.\n",
    "\n",
    "*pip install hadoop-azure*\n",
    "      \n",
    "*pip install azure-datalake-store*\n",
    "\n",
    "**Download the Required JAR Files:**\n",
    "\n",
    "Hadoop Azure JAR files are required for Spark to communicate with ADLS Gen2. Download the following JAR files and place them in a directory accessible to Spark (e.g., /path/to/jars):\n",
    "\n",
    "*hadoop-azure-3.2.1.jar*\n",
    "    \n",
    "*hadoop-azure-datalake-3.2.1.jar*\n",
    "    \n",
    "*azure-storage-7.0.0.jar*\n",
    "    \n",
    "*azure-data-lake-store-sdk-2.3.6.jar*\n",
    "\n",
    "You can download these JAR files from Maven repositories or directly from the Apache Hadoop website.\n",
    "\n",
    "**Configure PySpark to Use These JARs:**\n",
    "\n",
    "When starting PySpark, specify the location of these JAR files. This can be done by setting the PYSPARK_SUBMIT_ARGS environment variable before starting the Jupyter notebook.\n",
    "\n",
    "*export PYSPARK_SUBMIT_ARGS=\"--jars /path/to/jars/hadoop-azure-3.2.1.jar,/path/to/jars/hadoop-azure-datalake-3.2.1.jar,/path/to/jars/azure-storage-7.0.0.jar,/path/to/jars/azure-data-lake-store-sdk-2.3.6.jar pyspark-shell\"*\n",
    "\n",
    "**Start Jupyter Notebook with PySpark:**\n",
    "\n",
    "Ensure that you start Jupyter Notebook in an environment where the above environment variable is set.\n",
    "\n",
    "**Configure Spark Session in Jupyter Notebook:**\n",
    "\n",
    "In your Jupyter notebook, configure the Spark session with the necessary Hadoop configurations for ADLS Gen2.\n",
    "\n",
    "***`Setting PYSPARK_SUBMIT_ARGS - Globally`***\n",
    "\n",
    "**Edit Shell Profile:**\n",
    "\n",
    "Add the PYSPARK_SUBMIT_ARGS export statement to your shell profile configuration file. This can be ~/.bashrc, ~/.bash_profile, ~/.zshrc, or another appropriate configuration file depending on the shell you are using.\n",
    "\n",
    "For example, if you are using bash: nano ~/.bashrc and for zsh: nano ~/.zshrc\n",
    "\n",
    "**Add the Environment Variable:**\n",
    "\n",
    "*export PYSPARK_SUBMIT_ARGS=\"--jars /path/to/jars/hadoop-azure-3.2.1.jar,/path/to/jars/hadoop-azure-datalake-3.2.1.jar,/path/to/jars/azure-storage-7.0.0.jar,/path/to/jars/azure-data-lake-store-sdk-2.3.6.jar pyspark-shell\"*\n",
    "\n",
    "**Apply the Changes:**\n",
    "\n",
    "*source ~/.bashrc* or *source ~/.zshrc*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc577736-1526-4b0c-aea9-5bcf12922634",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark_context = SparkSession.builder.appName('IPL Data Analysis').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea738503-eb46-468b-aa1c-f688a8507fe9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee19353d-97fa-4f47-9479-ed2176545e26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Directly using the endpoint to access the data files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8232acf-b121-4c19-9a71-953798723be9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "application_id = 'c3bc59ef-a4ba-4a46-b547-66862ee4730d'\n",
    "directory_id = '97443b3e-100a-4ba4-947f-5e78ae387174'\n",
    "\n",
    "client_secret_id = '823b5971-5050-4585-a0b4-c48f4e84203a'\n",
    "client_secret = 'Bgo8Q~cd4gs467JXlJqCPfoC8S5S4xlTm.fYCc5B'\n",
    "\n",
    "oauth2_client_endpoint = \"https://login.microsoftonline.com/{}/oauth2/token\".format(directory_id)\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.adslgen2fortrainings.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.adslgen2fortrainings.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.adslgen2fortrainings.dfs.core.windows.net\", application_id)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.adslgen2fortrainings.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.adslgen2fortrainings.dfs.core.windows.net\", oauth2_client_endpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c5eb532-5ab9-4c3b-ac85-7dad111f3cc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Azure Storage Configuration\n",
    "storage_account_name = 'adslgen2fortrainings'\n",
    "storage_container_name = 'ipl-data-analysis'\n",
    "\n",
    "input_data_files_path = \"abfss://{}@{}.dfs.core.windows.net/input/{}\"\n",
    "output_data_files_path = \"abfss://{}@{}.dfs.core.windows.net/output/{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29dfcb12-343b-4c0a-8b25-320df396c27a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark Dataframe for 'ball_by_ball' dataset\n",
    "path_ball_by_ball = input_data_files_path.format(storage_container_name, storage_account_name, 'Ball_By_Ball.csv')\n",
    "\n",
    "schema_ball_by_ball = StructType([\n",
    "    StructField(\"match_id\", IntegerType(), False),\n",
    "    StructField(\"over_id\", IntegerType(), False),\n",
    "    StructField(\"ball_id\", IntegerType(), False),\n",
    "    StructField(\"innings_no\", IntegerType(), False),\n",
    "    StructField(\"team_batting\", StringType(), False),\n",
    "    StructField(\"team_bowling\", StringType(), False),\n",
    "    StructField(\"striker_batting_position\", IntegerType(), True),\n",
    "    StructField(\"extra_type\", StringType(), True),\n",
    "    StructField(\"runs_scored\", IntegerType(), True),\n",
    "    StructField(\"extra_runs\", IntegerType(), True),\n",
    "    StructField(\"wides\", IntegerType(), True),\n",
    "    StructField(\"legbyes\", IntegerType(), True),\n",
    "    StructField(\"byes\", IntegerType(), True),\n",
    "    StructField(\"noballs\", IntegerType(), True),\n",
    "    StructField(\"penalty\", IntegerType(), True),\n",
    "    StructField(\"bowler_extras\", IntegerType(), True),\n",
    "    StructField(\"out_type\", StringType(), True),\n",
    "    StructField(\"caught\", StringType(), True),\n",
    "    StructField(\"bowled\", StringType(), True),\n",
    "    StructField(\"run_out\", StringType(), True),\n",
    "    StructField(\"lbw\", StringType(), True),\n",
    "    StructField(\"retired_hurt\", StringType(), True),\n",
    "    StructField(\"stumped\", StringType(), True),\n",
    "    StructField(\"caught_and_bowled\", StringType(), True),\n",
    "    StructField(\"hit_wicket\", StringType(), True),\n",
    "    StructField(\"obstructingfeild\", StringType(), True),\n",
    "    StructField(\"bowler_wicket\", StringType(), False),\n",
    "    StructField(\"match_date\", DateType(), True),\n",
    "    StructField(\"season\", IntegerType(), True),\n",
    "    StructField(\"striker\", IntegerType(), True),\n",
    "    StructField(\"non_striker\", IntegerType(), True),\n",
    "    StructField(\"bowler\", IntegerType(), True),\n",
    "    StructField(\"player_out\", IntegerType(), True),\n",
    "    StructField(\"fielders\", IntegerType(), True),\n",
    "    StructField(\"striker_match_sk\", IntegerType(), True),\n",
    "    StructField(\"strikersk\", IntegerType(), True),\n",
    "    StructField(\"nonstriker_match_sk\", IntegerType(), True),\n",
    "    StructField(\"nonstriker_sk\", IntegerType(), True),\n",
    "    StructField(\"fielder_match_sk\", IntegerType(), True),\n",
    "    StructField(\"fielder_sk\", IntegerType(), True),\n",
    "    StructField(\"bowler_match_sk\", IntegerType(), True),\n",
    "    StructField(\"bowler_sk\", IntegerType(), True),\n",
    "    StructField(\"playerout_match_sk\", IntegerType(), True),\n",
    "    StructField(\"battingteam_sk\", IntegerType(), True),\n",
    "    StructField(\"bowlingteam_sk\", IntegerType(), True),\n",
    "    StructField(\"keeper_catch\", StringType(), True),\n",
    "    StructField(\"player_out_sk\", IntegerType(), True),\n",
    "    StructField(\"matchdatesk\", StringType(), True)\n",
    "])\n",
    "\n",
    "input_file_date_format = \"M/d/yyyy\"\n",
    "input_file_options = {\n",
    "    'delimiter': ',',\n",
    "    'header': 'True',\n",
    "    'inferSchema': 'False',\n",
    "    'dateFormat': input_file_date_format\n",
    "}\n",
    "df_ball_by_ball = spark_context.read \\\n",
    "    .format('csv') \\\n",
    "    .options(**input_file_options) \\\n",
    "    .schema(schema_ball_by_ball) \\\n",
    "    .load(path_ball_by_ball)\n",
    "\n",
    "df_ball_by_ball = \\\n",
    "    df_ball_by_ball.withColumns({\n",
    "        'caught': df_ball_by_ball.caught.cast(\"boolean\"),\n",
    "        'bowled': df_ball_by_ball.bowled.cast(\"boolean\"),\n",
    "        'run_out': df_ball_by_ball.run_out.cast(\"boolean\"),\n",
    "        'lbw': df_ball_by_ball.lbw.cast(\"boolean\"),\n",
    "        'retired_hurt': df_ball_by_ball.retired_hurt.cast(\"boolean\"),\n",
    "        'stumped': df_ball_by_ball.stumped.cast(\"boolean\"),\n",
    "        'caught_and_bowled': df_ball_by_ball.caught_and_bowled.cast(\"boolean\"),\n",
    "        'hit_wicket': df_ball_by_ball.hit_wicket.cast(\"boolean\"),\n",
    "        'obstructingfeild': df_ball_by_ball.obstructingfeild.cast(\"boolean\"),\n",
    "        'bowler_wicket': df_ball_by_ball.bowler_wicket.cast(\"boolean\"),\n",
    "        'keeper_catch': df_ball_by_ball.keeper_catch.cast(\"boolean\")\n",
    "    })\n",
    "df_ball_by_ball.show(5)\n",
    "display(df_ball_by_ball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1a69a2b-4a07-4b4e-9511-5a3d2e069d21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark Dataframe for 'match' dataset\n",
    "path_match = input_data_files_path.format(storage_container_name, storage_account_name, 'Match.csv')\n",
    "\n",
    "schema_match = StructType([\n",
    "    StructField(\"match_sk\", IntegerType(), True),\n",
    "    StructField(\"match_id\", IntegerType(), False),\n",
    "    StructField(\"team1\", StringType(), True),\n",
    "    StructField(\"team2\", StringType(), True),\n",
    "    StructField(\"match_date\", StringType(), True),\n",
    "    StructField(\"season_year\", IntegerType(), True),\n",
    "    StructField(\"venue_name\", StringType(), True),\n",
    "    StructField(\"city_name\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True),\n",
    "    StructField(\"toss_winner\", StringType(), True),\n",
    "    StructField(\"match_winner\", StringType(), True),\n",
    "    StructField(\"toss_name\", StringType(), True),\n",
    "    StructField(\"win_type\", StringType(), True),\n",
    "    StructField(\"outcome_type\", StringType(), True),\n",
    "    StructField(\"manofmach\", StringType(), True),\n",
    "    StructField(\"win_margin\", IntegerType(), True),\n",
    "    StructField(\"country_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "input_file_options = {\n",
    "    'delimiter': ',',\n",
    "    'header': 'True',\n",
    "    'inferSchema': 'False',\n",
    "    'dateFormat': input_file_date_format\n",
    "}\n",
    "df_match = spark_context.read \\\n",
    "    .format('csv') \\\n",
    "    .options(**input_file_options) \\\n",
    "    .schema(schema_match) \\\n",
    "    .load(path_match)\n",
    "\n",
    "df_match.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "341661d0-6f58-4110-ad36-fff82b991f3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark Dataframe for 'player' dataset\n",
    "path_player = input_data_files_path.format(storage_container_name, storage_account_name, 'Player.csv')\n",
    "\n",
    "schema_player = StructType([\n",
    "    StructField(\"player_sk\", IntegerType(), True),\n",
    "    StructField(\"player_id\", IntegerType(), False),\n",
    "    StructField(\"player_name\", StringType(), True),\n",
    "    StructField(\"dob\", StringType(), True),\n",
    "    StructField(\"batting_hand\", StringType(), True),\n",
    "    StructField(\"bowling_skill\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "input_file_date_format = 'M/d/yyyy'\n",
    "input_file_options = {\n",
    "    'delimiter': ',',\n",
    "    'header': 'True',\n",
    "    'inferSchema': 'False',\n",
    "    'dateFormat': input_file_date_format\n",
    "}\n",
    "df_player = spark_context.read \\\n",
    "    .format('csv') \\\n",
    "    .options(**input_file_options) \\\n",
    "    .schema(schema_player) \\\n",
    "    .load(path_player)\n",
    "\n",
    "df_player.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d87cd8ba-7ffd-4f1d-965a-5a09264e79c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark Dataframe for 'player_match' dataset\n",
    "path_player_match = input_data_files_path.format(storage_container_name, storage_account_name, 'Player_match.csv')\n",
    "\n",
    "schema_player_match = StructType([\n",
    "    StructField(\"player_match_sk\", IntegerType(), True),\n",
    "    StructField(\"playermatch_key\", DecimalType(), True),\n",
    "    StructField(\"match_id\", IntegerType(), False),\n",
    "    StructField(\"player_id\", IntegerType(), False),\n",
    "    StructField(\"player_name\", StringType(), True),\n",
    "    StructField(\"dob\", StringType(), True),\n",
    "    StructField(\"batting_hand\", StringType(), True),\n",
    "    StructField(\"bowling_skill\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True),\n",
    "    StructField(\"role_desc\", StringType(), True),\n",
    "    StructField(\"player_team\", StringType(), True),\n",
    "    StructField(\"opposit_team\", StringType(), True),\n",
    "    StructField(\"season_year\", IntegerType(), True),\n",
    "    StructField(\"is_manofthematch\", StringType(), True),\n",
    "    StructField(\"age_as_on_match\", IntegerType(), True),\n",
    "    StructField(\"isplayers_team_won\", StringType(), True),\n",
    "    StructField(\"batting_status\", StringType(), True),\n",
    "    StructField(\"bowling_status\", StringType(), True),\n",
    "    StructField(\"player_captain\", StringType(), True),\n",
    "    StructField(\"opposit_captain\", StringType(), True),\n",
    "    StructField(\"player_keeper\", StringType(), True),\n",
    "    StructField(\"opposit_keeper\", StringType(), True)\n",
    "])\n",
    "\n",
    "input_file_date_format = 'M/d/yyyy'\n",
    "input_file_options = {\n",
    "    'delimiter': ',',\n",
    "    'header': 'True',\n",
    "    'inferSchema': 'False',\n",
    "    'dateFormat': input_file_date_format\n",
    "}\n",
    "df_player_match = spark_context.read \\\n",
    "    .format('csv') \\\n",
    "    .options(**input_file_options) \\\n",
    "    .schema(schema_player_match) \\\n",
    "    .load(path_player_match)\n",
    "\n",
    "df_player_match = df_player_match.withColumns({\n",
    "    'is_manofthematch': df_player_match.is_manofthematch.cast('boolean'),\n",
    "    'isplayers_team_won': df_player_match.isplayers_team_won.cast('boolean')\n",
    "})\n",
    "\n",
    "df_player_match.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eecc0449-b8bc-49da-851d-81d6349870e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark Dataframe for 'team' dataset\n",
    "path_team = input_data_files_path.format(storage_container_name, storage_account_name, 'Player_match.csv')\n",
    "\n",
    "schema_team = StructType([\n",
    "    StructField(\"team_sk\", IntegerType(), True),\n",
    "    StructField(\"team_id\", IntegerType(), False),\n",
    "    StructField(\"team_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_team = spark_context.read \\\n",
    "    .format('csv') \\\n",
    "    .option('header', 'true') \\\n",
    "    .schema(schema_team) \\\n",
    "    .load(path_team)\n",
    "df_team.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a13ff0f1-9b44-42ba-bbb2-8bec62b8a178",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter to include only valid deliveries (excluding wides and no balls for specific analysis)\n",
    "df_ball_by_ball.filter((df_ball_by_ball.wides == 0) & (df_ball_by_ball.noballs == 0)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323b2d7e-a3b0-4802-81b0-cb0105e9ac6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the total runs scored in each match and inning : Aggregation\n",
    "df_ball_by_ball = \\\n",
    "    df_ball_by_ball.withColumn(\n",
    "        'ball_total_runs',\n",
    "        (df_ball_by_ball.runs_scored + df_ball_by_ball.extra_runs)\n",
    "    )\n",
    "\n",
    "df_innings_runs_scored = \\\n",
    "    df_ball_by_ball.groupBy(df_ball_by_ball.match_id, df_ball_by_ball.innings_no).agg(\n",
    "        F.sum(df_ball_by_ball.ball_total_runs).alias('innings_total_runs')\n",
    "    ).orderBy(df_ball_by_ball.match_id, df_ball_by_ball.innings_no)\n",
    "df_innings_runs_scored.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "635bd2d5-cc9a-458f-96de-2aa4c19060e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the average runs scored in each inning : Aggregation\n",
    "df_innings_runs_scored.groupBy(df_innings_runs_scored.innings_no).agg(\n",
    "    F.round(F.avg(df_innings_runs_scored.innings_total_runs)).alias('innings_avg_runs')\n",
    ").orderBy(df_innings_runs_scored.innings_no).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e382c252-32ee-4ab1-9a3a-5ec963ec5206",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the maximum runs scored in a super over\n",
    "super_over_innings_values = [3, 4]\n",
    "\n",
    "df_innings_runs_scored \\\n",
    "    .filter(df_innings_runs_scored.innings_no.isin(super_over_innings_values)) \\\n",
    "    .select(F.max(df_innings_runs_scored.innings_total_runs).alias('super_over_max_runs')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da3b21d9-3f17-4b83-a0f0-242b0f88cad6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Conditional Column: Flag for wicket (either a bowler_wicket or run_out or obstructing_field)\n",
    "df_ball_by_ball = df_ball_by_ball.withColumn(\n",
    "    \"wicket\",\n",
    "    F.when(\n",
    "        ((df_ball_by_ball.bowler_wicket == True) | (df_ball_by_ball.run_out == True) | (df_ball_by_ball.obstructingfeild == True)),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")\n",
    "df_ball_by_ball.filter(df_ball_by_ball.wicket == True).show(5)\n",
    "\n",
    "# Conditional Column: Flag for high impact balls (either a wicket or more than 6 runs including extras)\n",
    "df_ball_by_ball = df_ball_by_ball.withColumn(\n",
    "    \"high_impact\",\n",
    "    F.when(\n",
    "        (df_ball_by_ball.runs_scored + df_ball_by_ball.extra_runs > 6) | (df_ball_by_ball.wicket == True),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")\n",
    "df_ball_by_ball.filter(df_ball_by_ball.high_impact == False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "153e1ad6-5fed-4057-992a-7f18dac86cad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate running total of runs in each match for each ball : Window Functions\n",
    "windowSpec = Window.partitionBy(df_ball_by_ball.match_id, df_ball_by_ball.innings_no).orderBy(df_ball_by_ball.over_id, df_ball_by_ball.ball_id);\n",
    "\n",
    "col_innings_runs = F.sum(df_ball_by_ball.runs_scored + df_ball_by_ball.extra_runs).over(windowSpec).alias('innings_runs')\n",
    "col_innings_wickets = F.sum(F.when(df_ball_by_ball.wicket, 1).otherwise(0)).over(windowSpec).alias('innings_wickets')\n",
    "\n",
    "df_running_innings_run = \\\n",
    "    df_ball_by_ball.select(\n",
    "        df_ball_by_ball.match_id,\n",
    "        df_ball_by_ball.innings_no,\n",
    "        df_ball_by_ball.over_id,\n",
    "        df_ball_by_ball.ball_id,\n",
    "        df_ball_by_ball.runs_scored,\n",
    "        df_ball_by_ball.extra_runs,\n",
    "        col_innings_runs,\n",
    "        col_innings_wickets\n",
    "    )\n",
    "df_running_innings_run.filter((df_running_innings_run.match_id == 335987) & (df_running_innings_run.innings_no == 1)).show(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ad8a4f-bf64-4c43-ac7f-28ec5b37ea66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extracting year, month, and day from the match date for more detailed time-based analysis\n",
    "df_match = df_match.withColumns({\n",
    "    'match_year': F.year(df_match.match_date),\n",
    "    'match_month': F.month(df_match.match_date),\n",
    "    'match_day': F.dayofmonth(df_match.match_date)\n",
    "})\n",
    "df_match.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd1a12d-e831-46ad-8a19-8fcac9554281",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Win margin category: Categorizing win margins into 'high', 'medium', and 'low'\n",
    "df_match = df_match.withColumn(\n",
    "    'win_margin_category',\n",
    "    F.when(\n",
    "        ((df_match.win_type == 'runs') & (df_match.win_margin >= 100)) |\n",
    "        ((df_match.win_type == 'wickets') & (df_match.win_margin >= 5)),\n",
    "        'High'\n",
    "    ).when(\n",
    "        ((df_match.win_type == 'runs') & ((df_match.win_margin < 100) &  (df_match.win_margin >= 50))) |\n",
    "        ((df_match.win_type == 'wickets') & ((df_match.win_margin < 5) &  (df_match.win_margin >= 7))),\n",
    "        'Medium'\n",
    "    ).otherwise(\n",
    "        'Low'\n",
    "    )\n",
    ")\n",
    "df_match.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "729938d8-de5a-455a-b5a1-c460cc7b7341",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze the impact of the toss: who wins the toss and the match\n",
    "count_total_matches = df_match.count()\n",
    "print('Total number of matches: ' + str(count_total_matches))\n",
    "\n",
    "count_teams_winning_toss_and_match = df_match.filter(df_match.toss_winner == df_match.match_winner).count()\n",
    "print('Total number of matches where team won both the toss and match: ' + str(count_teams_winning_toss_and_match))\n",
    "\n",
    "impact_toss = (count_teams_winning_toss_and_match / count_total_matches) * 100\n",
    "print('Teams winning the toss won the match ' + str(impact_toss) + ' out of 100 times.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fdb8de7-f9d8-4e6a-9657-620acf062875",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Normalize and clean player names\n",
    "\n",
    "# Regular expression to match characters other than alphabets and numbers\n",
    "# \\w matches word characters (alphanumeric and underscore)\n",
    "pattern = r\"[^a-zA-Z0-9 ]\"\n",
    "\n",
    "# Players with names having matching the pattern\n",
    "df_player.filter(df_player.player_name.rlike(pattern)).show()\n",
    "\n",
    "# Replacing characters that are not in the pattern\n",
    "df_player = df_player.withColumn('player_name', F.lower(F.regexp_replace(df_player.player_name, '[^a-zA-Z0-9 ]', '')))\n",
    "\n",
    "# Players with names having matching the pattern\n",
    "df_player.filter(df_player.player_name.rlike(pattern)).show()\n",
    "\n",
    "# Handle missing values in 'batting_hand' and 'bowling_skill' with a default 'unknown'\n",
    "df_player_match.filter(df_player_match.batting_status.isNull() | df_player_match.bowling_status.isNull()).show(5)\n",
    "\n",
    "df_player_match = \\\n",
    "    df_player_match.fillna(\n",
    "        'unknown',\n",
    "        subset=['batting_status', 'bowling_status']\n",
    "    )\n",
    "\n",
    "df_player_match.filter(df_player_match.batting_status.isNull() | df_player_match.bowling_status.isNull()).show(5)\n",
    "df_player_match.filter(df_player_match.batting_status == 'unknown').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efa74178-3663-4338-bb79-ad21a8b7ec71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add a 'veteran' column based on player age\n",
    "df_player_match = df_player_match.withColumn(\n",
    "    'veteran',\n",
    "    F.when(df_player_match.age_as_on_match >= 35, True).otherwise(False)\n",
    ")\n",
    "df_player_match.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89038031-2402-436f-a7b7-276cdf4a8f7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create views for the Dataframes\n",
    "df_ball_by_ball.createOrReplaceGlobalTempView('view_ball_by_ball')\n",
    "df_match.createOrReplaceGlobalTempView('view_match')\n",
    "df_player.createOrReplaceGlobalTempView('view_player')\n",
    "df_player_match.createOrReplaceGlobalTempView('view_player_match')\n",
    "df_team.createOrReplaceGlobalTempView('view_team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00863fed-8d60-4a44-8add-afaac98eee6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Total runs scored by batsman per season\n",
    "\n",
    "df_season_batsman_runs = \\\n",
    "    df_ball_by_ball.join(\n",
    "        df_match,\n",
    "        df_ball_by_ball.match_id == df_match.match_id,\n",
    "        'inner'\n",
    "    ).join(\n",
    "        df_player_match,\n",
    "        (df_match.match_id == df_player_match.match_id) & (df_ball_by_ball.striker == df_player_match.player_id),\n",
    "        'inner'\n",
    "    ).join(\n",
    "        df_player,\n",
    "        df_player_match.player_id == df_player.player_id,\n",
    "        'inner'\n",
    "    ).groupBy(df_match.season_year, df_player.player_id).agg(\n",
    "        F.sum(df_ball_by_ball.runs_scored).alias('season_runs')\n",
    "    )\n",
    "\n",
    "df_season_batsman_runs = \\\n",
    "    df_season_batsman_runs.alias('batsman').join(\n",
    "        df_player.alias('player'),\n",
    "        F.col('batsman.player_id') == F.col('player.player_id'),\n",
    "        'inner'\n",
    "    ).select(\n",
    "        F.col('batsman.season_year'),\n",
    "        F.col('player.player_id'),\n",
    "        F.col('player.player_name'),\n",
    "        F.col('batsman.season_runs')\n",
    "    ).orderBy(\n",
    "        F.col('batsman.season_year'), F.col('batsman.season_runs').desc()\n",
    "    )\n",
    "\n",
    "df_season_batsman_runs.show(10)\n",
    "\n",
    "# Save total runs scored by batsman per season\n",
    "path_season_batsman_runs = output_data_files_path.format(storage_container_name, storage_account_name, 'season_batsman_runs')\n",
    "output_file_options = {\n",
    "    'delimiter': ',',\n",
    "    'header': 'True'\n",
    "}\n",
    "df_season_batsman_runs.write.format('csv').mode('overwrite').options(**output_file_options).save(path_season_batsman_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4610ec8-50e7-4c58-bbb1-6a3d15547bac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Orange cap holder per season (Top scoring batsman)\n",
    "windowSpec = Window.partitionBy(df_season_batsman_runs.season_year).orderBy(df_season_batsman_runs.season_runs.desc())\n",
    "\n",
    "df_season_batsman_runs.select(\n",
    "    df_season_batsman_runs.season_year,\n",
    "    df_season_batsman_runs.player_id,\n",
    "    df_season_batsman_runs.player_name,\n",
    "    df_season_batsman_runs.season_runs,\n",
    "    F.rank().over(windowSpec).alias('rank')\n",
    ").filter(F.col('rank') == 1).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ipl_data_analysis_1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
